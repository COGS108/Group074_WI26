{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe9d8f-b0a0-4e97-9abd-014889a56f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39ce18-5ee9-42f1-8748-1992b381157f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_url = \"https://www.hoopshype.com/salaries/players/\"\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(data_url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html)\n",
    "for tag in soup.find_all('tr'):\n",
    "    print(tag.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564b8c3-0de5-48b1-83ae-e43361fdfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://www.hoopshype.com/salaries/players/\"\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(data_url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dda443-23c8-4827-9519-d2b01a39e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_buttons = driver.find_elements(By.CLASS_NAME,\"ULq-fz__ULq-fz\")\n",
    "year_buttons[0].click()\n",
    "all_years = driver.find_elements(By.CLASS_NAME,\"_2qaD9L__2qaD9L\")\n",
    "# for yr in all_years:\n",
    "#     yr.click()\n",
    "#     time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0f6e8-630e-40e8-90aa-11a307cf4fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_urls = [yr.get_attribute(\"href\") for yr in all_years]\n",
    "wait = WebDriverWait(driver, 10) \n",
    "data_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5f683-ce84-46ec-9621-1a34c257c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = pd.DataFrame()\n",
    "row_idx = 0\n",
    "MAX_RETRIES = 7\n",
    "teams = {\n",
    "    1:  \"Atlanta Hawks\",\n",
    "    2:  \"Boston Celtics\",\n",
    "    3:  \"New Orleans Pelicans\",\n",
    "    4:  \"Chicago Bulls\",\n",
    "    5:  \"Cleveland Cavaliers\",\n",
    "    6:  \"Dallas Mavericks\",\n",
    "    7:  \"Denver Nuggets\",\n",
    "    8:  \"Detroit Pistons\",\n",
    "    9:  \"Golden State Warriors\",\n",
    "    10: \"Houston Rockets\",\n",
    "    11: \"Indiana Pacers\",\n",
    "    12: \"Los Angeles Clippers\",\n",
    "    13: \"Los Angeles Lakers\",\n",
    "    14: \"Miami Heat\",\n",
    "    15: \"Milwaukee Bucks\",\n",
    "    16: \"Minnesota Timberwolves\",\n",
    "    17: \"Brooklyn Nets\",\n",
    "    18: \"New York Knicks\",\n",
    "    19: \"Orlando Magic\",\n",
    "    20: \"Philadelphia 76ers\",\n",
    "    21: \"Phoenix Suns\",\n",
    "    22: \"Portland Trail Blazers\",\n",
    "    23: \"Sacramento Kings\",\n",
    "    24: \"San Antonio Spurs\",\n",
    "    25: \"Oklahoma City Thunder\",\n",
    "    26: \"Utah Jazz\",\n",
    "    27: \"Washington Wizards\",\n",
    "    28: \"Toronto Raptors\",\n",
    "    29: \"Memphis Grizzlies\",\n",
    "    53: \"Charlotte Hornets\",\n",
    "}\n",
    "\n",
    "\n",
    "for data_url in data_urls[:8]:\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(data_url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    table_pg_1 = soup.find_all('tr')\n",
    "\n",
    "    headers_raw = table_pg_1[0].find_all(class_=\"RLrCiX__RLrCiX\")\n",
    "    headers = [header.text for header in headers_raw]\n",
    "    page_num_count = 3\n",
    "    while page_num_count != 2:\n",
    "        retries = 0\n",
    "        page_success = False\n",
    "\n",
    "        while retries < MAX_RETRIES and not page_success:\n",
    "            try:\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html)\n",
    "                table = soup.find_all('tr')\n",
    "                page_num = set(driver.find_elements(By.CLASS_NAME,\"l98ZPp__l98ZPp\")[0].text.split())\n",
    "                page_num_count = len(page_num)\n",
    "                print(page_num)\n",
    "\n",
    "                for player in table[1:]:\n",
    "                    try:\n",
    "                        player_salaries = player.find_all(class_=\"RLrCiX__RLrCiX\")\n",
    "                        name = player.find(class_=\"_0cD6l-__0cD6l-\").text\n",
    "                        team_logo = player.find('img')\n",
    "                        team_num_raw = team_logo.get('src').split(\"/\")[7] \n",
    "                        team_num = int(team_num_raw[0]) if team_num_raw[1] == \".\" else int(team_num_raw[:2])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing player row: {e}\")\n",
    "                        retries += 1\n",
    "                        print(f\"Error processing page (attempt {retries}/{MAX_RETRIES}): {e}\")\n",
    "                        time.sleep(2)\n",
    "                        if retries >= MAX_RETRIES:\n",
    "                            print(\"Max retries reached for this page, skipping.\")\n",
    "                        \n",
    "                    salary_df.loc[row_idx, \"name\"] = name\n",
    "                    salary_df.loc[row_idx, \"team\"] = teams[team_num]\n",
    "                    for i in range(len(player_salaries)):\n",
    "                        salary_df.loc[row_idx, headers[i]] = player_salaries[i].text\n",
    "                    row_idx += 1\n",
    "\n",
    "                page_success = True\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error processing page (attempt {retries}/{MAX_RETRIES}): {e}\")\n",
    "                time.sleep(2)\n",
    "                if retries >= MAX_RETRIES:\n",
    "                    print(\"Max retries reached for this page, skipping.\")\n",
    "\n",
    "        print(salary_df.shape)\n",
    "        print(salary_df.iloc[-5:])\n",
    "\n",
    "        try:\n",
    "            buttons = driver.find_elements(By.CLASS_NAME, \"hd3Vfp__hd3Vfp\")\n",
    "            buttons[1].click()\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error processing page (attempt {retries}/{MAX_RETRIES}): {e}\")\n",
    "            time.sleep(2)\n",
    "            if retries >= MAX_RETRIES:\n",
    "                print(\"Max retries reached for this page, skipping.\")\n",
    "\n",
    "\n",
    "        time.sleep(2.5)\n",
    "\n",
    "    driver.quit()\n",
    "    salary_df.to_csv(\"data/00-raw/salary_data_raw.csv\", index=False)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946b4a1-8f6c-406b-9a89-136d5a870a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
